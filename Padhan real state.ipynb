import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
real = pd.read_excel(r"C:\Users\LENOVO\OneDrive\Desktop\Machine\Real estate valuation data set.xlsx")

real.head()
real.info()
real['X2 house age'].value_counts()
real.describe()
%matplotlib inline
real.hist(bins=50, figsize=(20, 15))
Train-Test Splitting

def split_train_test(data, test_ratio):
    np.random.seed(50)
    shuffled = np.random.permutation(len(data))
    print(shuffled)
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled[:test_set_size]
    train_indices = shuffled[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = split_train_test(real, 0.2)
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(real, test_size=0.2, random_state=50)
print(f"Rows in train set: {len(train_set)}\nRows in test set: {len(train_set)}\n")


from sklearn.model_selection import StratifiedShuffleSplit

# Create a new column with binned values for stratification
real["MRT_dist_cat"] = pd.cut(real["X3 distance to the nearest MRT station"],
                              bins=[0, 1000, 2000, 3000, 4000, float("inf")],
                              labels=[1, 2, 3, 4, 5])

# Now use stratified sampling on the new categorical column
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=50)

for train_index, test_index in split.split(real, real["MRT_dist_cat"]):
    strat_train_set = real.loc[train_index]
    strat_test_set = real.loc[test_index]

# Optionally drop the helper column
for set_ in (strat_train_set, strat_test_set):
    set_.drop("MRT_dist_cat", axis=1, inplace=True)

strat_test_set['X1 transaction date']
Looking for Correlations
corr_matrix = real.corr()
corr_matrix['Y house price of unit area'].sort_values(ascending=False)
strat_train_set["Y house price of unit area"].value_counts()

real = strat_train_set.copy()
from pandas.plotting import scatter_matrix

attributes = ['Y house price of unit area', 'X1 transaction date', 'X5 latitude', 'X6 longitude']
scatter_matrix(real[attributes], figsize=(12, 8))

real.plot(kind="scatter", x="X5 latitude", y="X6 longitude", alpha=0.1)

# Remove the target column from features
real = strat_train_set.drop("X4 number of convenience stores", axis=1)

# Separate the target column as labels
real_labels = strat_train_set["X4 number of convenience stores"].copy()

## Attribute Combinations

real["X3 distance to the nearest MRT stationX4 number of convonience store"] = (
    strat_train_set['X3 distance to the nearest MRT station'] / 
    strat_train_set["X4 number of convenience stores"]
)

real.head()
strat_train_set = strat_train_set.dropna(subset=["X4 number of convenience stores"])

print(real.columns.tolist())

median = strat_train_set["X4 number of convenience stores"].median()

median = strat_train_set["X4 number of convenience stores"].median()
strat_train_set["X4 number of convenience stores"].fillna(median, inplace=True)

real.shape
real = strat_train_set.copy()

real.describe()
import numpy as np
from sklearn.impute import SimpleImputer

# 1. Select only numeric columns
real_num = real.select_dtypes(include=["number"])

# 2. Replace infinite values with NaN
real_num.replace([np.inf, -np.inf], np.nan, inplace=True)

# 3. Apply median imputation
imputer = SimpleImputer(strategy="median")
imputer.fit(real_num)

imputer.statistics_.shape


# 1. Select numeric columns
real_num = real.select_dtypes(include=["number"])

# 2. Replace inf and -inf with NaN
real_num.replace([np.inf, -np.inf], np.nan, inplace=True)

# 3. Fit the imputer
imputer = SimpleImputer(strategy="median")
imputer.fit(real_num)

# 4. Transform the data
X = imputer.transform(real_num)

real.describe()
Scikit-learn Design

Creating a Pipeline

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

my_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('std_scalar', StandardScaler()),
])

real_num_tr = my_pipeline.fit_transform(real_num)
real_num_tr.shape
Selecting a desired model for Padhan real estate

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
#model = DecisionTreeRegressor()
#model = LinearRegression()


#model.fit(real_num[:15], real_labels[:15])
 
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(real_num_tr, real_labels)  # make sure these variables are defined

some_data = real.iloc[:5]
some_data = real_labels.iloc[:5]
some_data = real.iloc[[0]]  # This is a DataFrame (2D)
prepared_data = my_pipeline.transform(some_data)

model.predict(prepared_data)
some_label = real_labels.iloc[0]   # this is a scalar
print(some_label)

Evaluting the model
from sklearn.metrics import mean_squared_error

real_predictions = model.predict(real_num_tr)
lin_mse = mean_squared_error(real_labels, real_predictions)
lin_mse = np.sqrt(lin_mse)
lin_mse
 using better evaluation technique - Cross Validation
from sklearn.model_selection import cross_val_score
import numpy as np  # ensure this is imported

scores = cross_val_score(model, real_num_tr, real_labels, scoring="neg_mean_squared_error", cv=15)
rmse_scores = np.sqrt(-scores)

rmse_scores
def print_scores(scores):
    print("RMSE scores:", rmse_scores)
print("Mean:", rmse_scores.mean())
print("Standard deviation:", rmse_scores.std())
print_scores(rmse_scores)
Saving the model
from joblib import dump

# Step 1: Create the model
clf = RandomForestRegressor()

# Step 2: Fit the model (make sure real_num_tr and real_labels are already defined)
clf.fit(real_num_tr, real_labels)

# Step 3: Save the model
dump(clf, "Dragon.joblib")
Testing the model on Test data
real = strat_train_set.drop("X4 number of convenience stores", axis=1)
real_labels = strat_train_set["X4 number of convenience stores"].copy()
my_pipeline.fit(real)
real_predictions = model.predict(real_num_tr)
lin_mse = mean_squared_error(real_labels, real_predictions)
lin_mse = np.sqrt(lin_mse)
print(lin_mse)
